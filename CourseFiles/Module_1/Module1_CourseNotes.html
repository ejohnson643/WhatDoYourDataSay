
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Module 1 Course Notes &#8212; What Do Your Data Say?</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" href="../../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Worksheet 1.1: Coin Tossing" href="Worksheet_1_1_CoinFlipping.html" />
    <link rel="prev" title="Module 1: The Basics" href="Module_1.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../../_static/PCA_Animation_WDYDS.gif" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">What Do Your Data Say?</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../intro.html">
                    What Do Your Data Say?
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Resources.html">
   Course Resources
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../AboutUs.html">
     About Us
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../CurriculumAlignmentTables.html">
     Curriculum Alignment Tables
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Rubric.html">
     Rubric
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../HowTo_AssignmentAttempt.html">
     How-To: Make an Assignment Attempt
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../HowTo_AssignmentCompletion.html">
     How-To: Complete Assignments
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../HowTo_SelfAssessment.html">
     How-To: Self-Assessment
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="../Modules.html">
   Modules
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="../Module_0/Module_0.html">
     Module 0: Python Tutorial
    </a>
   </li>
   <li class="toctree-l2 current active has-children">
    <a class="reference internal" href="Module_1.html">
     Module 1: The Basics
    </a>
    <input checked="" class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
    <label for="toctree-checkbox-3">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul class="current">
     <li class="toctree-l3 current active">
      <a class="current reference internal" href="#">
       Module 1 Course Notes
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="Worksheet_1_1_CoinFlipping.html">
       Worksheet 1.1: Coin Tossing
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="Worksheet_1_1_CoinFlipping_Solutions.html">
       SOLUTIONS to Worksheet 1.1: Coin Tossing
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="Worksheet_1_2_SquareRootN.html">
       Worksheet 1.2: The Spread in Distributions
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="Worksheet_1_2_SquareRootN_Guide.html">
       GUIDE to Worksheet 1.2: The Spread in Distributions
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="Worksheet_1_3_EffectOfPriors.html">
       Worksheet 1.3: Bayes’ Theorem and the Effect of Priors
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="Worksheet_1_3_EffectOfPriors_Guide.html">
       GUIDE to Worksheet 1.3: Bayes’ Theorem and the Effect of Priors
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="Assignment_1.html">
       Assignment 1
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Module_2/Module_2.html">
     Module 2
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Module_3/Module_3.html">
     Module 3
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Module_4/Module_4.html">
     Module 4
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../markdown.html">
   Markdown Files
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../markdown-notebooks.html">
   Notebooks with MyST Markdown
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../notebooks.html">
   Content with notebooks
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/ejohnson643/WhatDoYourDataSay"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/ejohnson643/WhatDoYourDataSay/issues/new?title=Issue%20on%20page%20%2FCourseFiles/Module_1/Module1_CourseNotes.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../../_sources/CourseFiles/Module_1/Module1_CourseNotes.md"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.md</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#coding-and-plotting">
   Coding and Plotting
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-atom-of-probability">
   The Atom of Probability
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#basic-probability-theory">
     Basic Probability Theory
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#flipping-coins">
     Flipping Coins
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#why-are-coin-tosses-random">
       Why are Coin Tosses Random?
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#flipping-several-coins">
     Flipping Several Coins
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#features-of-distributions">
     Features of Distributions
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#moments-of-distributions">
       Moments of Distributions
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#cumulative-distribution-functions">
       Cumulative Distribution Functions
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-central-limit-theorem">
     The Central Limit Theorem
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-atom-of-bayesian-probability">
   The Atom of Bayesian Probability
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#example-bayesian-coin-tossing">
     Example: Bayesian Coin Tossing
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#why-bayes">
     Why Bayes?
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#bacterial-chemotaxis">
   Bacterial Chemotaxis
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#review">
   Review
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#learning-goals">
   Learning Goals
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#other-details">
   Other Details
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#more-probability-rules">
     More Probability Rules
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#the-addition-rule">
       The Addition Rule
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#the-multiplication-rule">
       The Multiplication Rule
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#the-complement-rule">
       The Complement Rule
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#law-of-total-probability">
       Law of Total Probability
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#marginal-distributions">
       Marginal Distributions
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#more-theoretical-distributions">
     More Theoretical Distributions
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#the-binomial-distribution">
       The Binomial Distribution
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#the-gaussian-distribution">
       The Gaussian Distribution
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#the-poisson-distribution">
       The Poisson Distribution
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#the-exponential-distribution">
       The Exponential Distribution
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Module 1 Course Notes</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#coding-and-plotting">
   Coding and Plotting
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-atom-of-probability">
   The Atom of Probability
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#basic-probability-theory">
     Basic Probability Theory
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#flipping-coins">
     Flipping Coins
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#why-are-coin-tosses-random">
       Why are Coin Tosses Random?
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#flipping-several-coins">
     Flipping Several Coins
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#features-of-distributions">
     Features of Distributions
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#moments-of-distributions">
       Moments of Distributions
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#cumulative-distribution-functions">
       Cumulative Distribution Functions
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-central-limit-theorem">
     The Central Limit Theorem
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-atom-of-bayesian-probability">
   The Atom of Bayesian Probability
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#example-bayesian-coin-tossing">
     Example: Bayesian Coin Tossing
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#why-bayes">
     Why Bayes?
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#bacterial-chemotaxis">
   Bacterial Chemotaxis
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#review">
   Review
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#learning-goals">
   Learning Goals
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#other-details">
   Other Details
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#more-probability-rules">
     More Probability Rules
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#the-addition-rule">
       The Addition Rule
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#the-multiplication-rule">
       The Multiplication Rule
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#the-complement-rule">
       The Complement Rule
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#law-of-total-probability">
       Law of Total Probability
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#marginal-distributions">
       Marginal Distributions
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#more-theoretical-distributions">
     More Theoretical Distributions
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#the-binomial-distribution">
       The Binomial Distribution
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#the-gaussian-distribution">
       The Gaussian Distribution
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#the-poisson-distribution">
       The Poisson Distribution
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#the-exponential-distribution">
       The Exponential Distribution
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="module-1-course-notes">
<h1>Module 1 Course Notes<a class="headerlink" href="#module-1-course-notes" title="Permalink to this headline">#</a></h1>
<p>Before we can really talk about building and assessing statistical models with data, you will need to be familiar with some basics of probability theory, coding, and simulation.  This module will provide these basics as well as motivate why our framework for confronting data from a statistical viewpoint is justified.  You will quickly see that while some of these basics resemble what you might learn in an introductory statistics textbook, our emphasis will reside much more on the practical and intuitive aspects of probability rather than on developing a series of rules and tests.  In particular, we believe that a modern quantitative thinker knows not just the <strong>theory</strong> of what a probability density function is (for example), but also how to <strong>generate</strong> one from data or simulations, how to <strong>visualize</strong> one, and how to <strong>manipulate</strong> one to make different calculations.</p>
<p>These notes have three sections: the first section will simply link to the Python tutorial that should be completed before/during the reading of this text.  The next two sections will then introduce basic probability concepts, such as probability and frequency distributions, and Bayes Theorem, respectively.  Each section will emphasize <strong>theory, calculation, visualization</strong>, and <strong>simulation</strong> somewhat evenly.  At the end of the chapter will be a reference section summarizing the main takeaways as well as providing some details on technical aspects that are not discussed in the main part of the chapter.\</p>
<div class="admonition-big-idea admonition">
<p class="admonition-title">Big Idea</p>
<p>Data is <em>distributional</em> because of inherent randomness in the physical world.  We can visualize data distributions in our computers and use both theory and simulation to compute interesting quantities using these distributions.</p>
</div>
<section id="coding-and-plotting">
<h2>Coding and Plotting<a class="headerlink" href="#coding-and-plotting" title="Permalink to this headline">#</a></h2>
<p>As mentioned above, we believe that a modern quantitative researcher knows not just how to run programs and canned scripts, but more generically how to use and generate computer code as a tool that supplements all aspects of quantitative experimentation.  The Python tutorial linked <a class="reference external" href="https://ejohnson643.github.io/PythonTutorial">here</a> contains the minimum coding details needed to complete the assignments and worksheets contained in this course.  While a completely novice coder may struggle to quickly complete these tasks, it is our goal that there is sufficient support in these resources that anyone can follow along.</p>
<p>In particular, to complete the worksheets and “Try It Yourself” tasks, you should complete the tutorial except for the sections on dictionaries and classes.  The sections on random numbers, plotting, and loops will be especially useful in this module.</p>
</section>
<section id="the-atom-of-probability">
<h2>The Atom of Probability<a class="headerlink" href="#the-atom-of-probability" title="Permalink to this headline">#</a></h2>
<p>While we hope to convince you that the methods and frameworks presented in this text will at least be useful, it is worth spending some time arguing that a probabilistic view of the world is actually the <em>correct</em> view in that it most accurately describes and incorporates all observed phenomena.  This may sound philosophical, and it is, but it is also practical.  If you are a generic empirical observer this practicality is two-fold.</p>
<p>First, let’s say that you have an experimental measurement that you believe is <em>deterministic</em> (whatever that may mean), this measurement is inevitably corrupted by measurement error, where we refer to error intuitively as a discrepancy between what we observe and the true value of the phenomenon we are observing.  This error might be due to the measurer, the precision of the measuring instrument, or some other unknown effects.  Secondly, and more directly, any physicist or chemist can attest that <em>all</em> phenomena are stochastic (random), either due to thermal or quantum mechanical effects.  The relevance of this stochasticity depends on the phenomenon, but generally as the scale of the phenomenon becomes smaller (in time, size, number of samples, etc.), the more important this physical randomness becomes. These physical considerations place fundamental limits of reproducibility in the phenomena and measuring it.</p>
<p>Given the inherent noisiness or error in all measurements, it should seem necessary to have methods to account for it.  That is, given that I know that there is some element of randomness in my observations, how can I make inferences or conclusions from any measurement?  Probability theory was created precisely to provide a framework in which to understand the inevitable randomness in any real measurement.</p>
<section id="basic-probability-theory">
<h3>Basic Probability Theory<a class="headerlink" href="#basic-probability-theory" title="Permalink to this headline">#</a></h3>
<p>While we won’t spend much time worrying about the theory, it is useful to spell out a few basics.  In particular, the base premise of probability theory is that all events are <strong>random variables</strong> that take on different values with different **likelihoods}.  That is, the roll of a single die is a random event where the value of 1 occurs with likelihood <span class="math notranslate nohighlight">\(1/6\)</span>, the value 2 occurs with likelihood <span class="math notranslate nohighlight">\(1/6\)</span>, etc.  Mathematically, we use the notation of a capital letter, <span class="math notranslate nohighlight">\(X\)</span> for example, to indicate a random variable, and its lower case, <span class="math notranslate nohighlight">\(x\)</span>, to indicate a specific outcome of that random event.  So in the example, <span class="math notranslate nohighlight">\(X\)</span> indicates the roll of a die, and <span class="math notranslate nohighlight">\(x=1\)</span> is a specific outcome.  To make statements about likelihood, we then can write</p>
<div class="math notranslate nohighlight">
\[P(X = 1) = 1/6,\]</div>
<p>which is read, “the probability that the outcome of a dice roll is 1 is one out of six (<span class="math notranslate nohighlight">\(1/6 \approx 16.667\%\)</span>).”  The <span class="math notranslate nohighlight">\(\mathbf{P()}\)</span> indicates that we’re making a statement about the likelihood or <em>probability</em> of whatever is being written in the parentheses.  We we write this generically as <span class="math notranslate nohighlight">\(P(X = x)\)</span> and we will often use the abbreviated form <span class="math notranslate nohighlight">\(P(X)\)</span> to mean the same thing.</p>
<p>A modern statistics textbook would then go on to elaborate on some rules of probability, but these tend to be more details than essential to an understanding of the topic, so we’ll only discuss them at the end of the chapter.  However, it is worth thinking briefly about the concept of <strong>dependent</strong> and <strong>independent</strong> random events.  These definitions are mostly intuitive; two random events are independent if their outcomes are independent of one another, they are dependent if one of them depends on the other.  For example, if I have two coins and I flip one, the likelihood that the other one will be heads doesn’t depend on the outcome of the first flip.  On the other hand, if I have a deck of cards and I draw two cards, keeping track of the order in which they were drawn, the likelihood that the second card is the ace of spades <em>depends</em> on whether I drew the ace of spades as my first card.</p>
<p>Mathematically, independent random events have the property that their likelihoods <em>multiply</em>:</p>
<div class="math notranslate nohighlight" id="equation-eqn-indepdefn">
<span class="eqno">(1)<a class="headerlink" href="#equation-eqn-indepdefn" title="Permalink to this equation">#</a></span>\[P(X\text{  AND  }Y) = P(X)\times P(Y)\]</div>
<p>To be explicit, <span class="math notranslate nohighlight">\(P(X\text{AND}Y)\)</span> is  the <strong>joint probability distribution</strong> of <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>.  Note that this is the abbreviated notation, and what we mean when we write this is that we are interested in the likelihood that <span class="math notranslate nohighlight">\(X\)</span> takes on some outcome, <span class="math notranslate nohighlight">\(x\)</span>, <em>and</em> that <span class="math notranslate nohighlight">\(Y\)</span> takes on a specific outcome, <span class="math notranslate nohighlight">\(y\)</span>, so that we might also write <span class="math notranslate nohighlight">\(P(X = x \text{AND} Y = y)\)</span>.  In this way, can unpack Equation <a class="reference internal" href="#equation-eqn-indepdefn">(1)</a> in words: if <span class="math notranslate nohighlight">\(X\)</span> is the roll of a die and <span class="math notranslate nohighlight">\(Y\)</span> is the flip of a coin, then the likelihood of rolling a 1 and flipping a heads should just be</p>
<div class="math notranslate nohighlight">
\[P(X = 1 \text{  AND  } Y = H) = \frac{1}{6}\times \frac{1}{2} = \frac{1}{12}.\]</div>
<p>Hopefully you can see that this holds in general: if two things don’t depend on each other, then the likelihood of both of them can be found by multiplying their individual likelihoods.</p>
<p>To see why this is a useful consideration to make, we introduce the concept of a <strong>conditional probability</strong>, which we denote <span class="math notranslate nohighlight">\(P(X|Y)\)</span>.  In words, we read this as “the probability of random variable <span class="math notranslate nohighlight">\(X\)</span> taking on a specific outcome <em>given</em> a specific outcome for <span class="math notranslate nohighlight">\(Y\)</span>.” So in the example of drawing cards, if <span class="math notranslate nohighlight">\(C_1\)</span> is the first card I draw and <span class="math notranslate nohighlight">\(C_2\)</span> is the second, then the probability of <span class="math notranslate nohighlight">\(C_2 = \)</span> ace of spades can be notated</p>
<div class="math notranslate nohighlight">
\[\begin{split}P(C_2 = \text{ace of spades} | C_1) =
\begin{cases}
    0 &amp;\quad \text{if } C_1 = \text{ace of spades}\\
    \frac{1}{51} &amp;\quad \text{if } C_1 \neq \text{ace of spades}.
\end{cases}\end{split}\]</div>
<p>So <em>given</em> the first card that I drew from the deck, the probability of the outcome of the second card changes.  This is completely different from two independent random events, where the probability of flipping heads, for example, doesn’t depend on whether I just flipped heads or tails (or rolled a die, or measured the temperature, or went for a walk, etc.).</p>
<p>If we then want to know the <em>joint probability</em> of getting two specific cards <span class="math notranslate nohighlight">\(C_1\)</span> and <span class="math notranslate nohighlight">\(C_2\)</span>, then we can write the joint distribution more generally as</p>
<div class="math notranslate nohighlight" id="equation-eqn-jointdist">
<span class="eqno">(2)<a class="headerlink" href="#equation-eqn-jointdist" title="Permalink to this equation">#</a></span>\[P(C_2, C_1) = P(C_2 | C_1)\times P(C_1),\]</div>
<p>where we’ve now used the further abbreviation of <span class="math notranslate nohighlight">\(P(C_2, C_1)\)</span> to refer to the joint distribution of <span class="math notranslate nohighlight">\(C_1\)</span> and <span class="math notranslate nohighlight">\(C_2\)</span>.  This equation is the general definition of a joint probability distribution, regardless of whether <span class="math notranslate nohighlight">\(C_1\)</span> and <span class="math notranslate nohighlight">\(C_2\)</span> are independent.  If <span class="math notranslate nohighlight">\(C_1\)</span> and <span class="math notranslate nohighlight">\(C_2\)</span> were independent, then Equations <a class="reference internal" href="#equation-eqn-indepdefn">(1)</a> and <a class="reference internal" href="#equation-eqn-jointdist">(2)</a> would be equivalent because for independent random variables</p>
<div class="math notranslate nohighlight">
\[P(C_2 | C_1) = P(C_2),\]</div>
<p>as we’d expect.  That is, if they are independent, then the probability of <span class="math notranslate nohighlight">\(C_2\)</span> does not depend on <span class="math notranslate nohighlight">\(C_1\)</span>.  (The above equation is an alternate definition of independence.)</p>
<p>This might all sound somewhat tautological, but it can be somewhat subtle and it will continue to come up as you progress through the course.  Most importantly, we want to get you used to the notation and how you should read these mathematical statements to yourself. In particular, you should make sure you understand the difference between two events being independent or not.  Two measurements being independent does not just mean that they were taken with different instruments or by different people.  If I measure the temperature at noon and you measure the humidity 4 hours later in the same location, those quantities are not independent; the likelihood of 60% humidity definitely depends on whether it was <span class="math notranslate nohighlight">\(0^{\circ}\)</span>F or <span class="math notranslate nohighlight">\(100^{\circ}\)</span>F at noon.  (If this weren’t the case, we’d have no ability to predict the weather at all!)</p>
<p>As a result, you should consider independence to be the exception rather than the rule, and we prefer our quantities to be independent for mathematical and theoretical reasons.  Generally, we assume that our data are composed of independent samples (of one or more quantities that may or may not be independent of each other), but for many of the methods presented in these notes this is not essential.  We will attempt to be clear when independence is an implicit assumption to any method.</p>
</section>
<section id="flipping-coins">
<h3>Flipping Coins<a class="headerlink" href="#flipping-coins" title="Permalink to this headline">#</a></h3>
<p>Now that we have gotten some formality out of the way, let’s get to our first big concept: flipping coins.  The toss of a coin is often called the “hydrogen atom” of probability and statistics because just as a hydrogen atom is the building block for theory in physics and chemistry, the coin toss is our elemental unit.</p>
<p>To be precise, let’s consider a coin toss to be a random event, <span class="math notranslate nohighlight">\(X\)</span>, with two possible outcomes: heads (H) and tails (T).  Let’s say that the likelihood of getting a heads is <span class="math notranslate nohighlight">\(p\)</span> so that</p>
<div class="math notranslate nohighlight">
\[P(X=H) = p\qquad\text{and}\qquad P(X=T) = 1-p.\]</div>
<p>Why must <span class="math notranslate nohighlight">\(P(X=T)=1-p\)</span>?  This is a (hopefully intuitive) rule that the probability that <em>any outcome</em> will result from a random event is 100%.  If I flip a coin, I’ll get heads or tails; something has to happen.  So if the likelihood of heads is <span class="math notranslate nohighlight">\(p=4/10\)</span>, then since tails is the only other option, <span class="math notranslate nohighlight">\(P(X=T)=6/10\)</span>.  This is a useful property that we’ll make use of frequently.</p>
<section id="why-are-coin-tosses-random">
<h4>Why are Coin Tosses Random?<a class="headerlink" href="#why-are-coin-tosses-random" title="Permalink to this headline">#</a></h4>
<p>Now, while you’ve probably been told your whole life that a coin toss has a random outcome, you may be wondering how exactly (physically) this can be.  To better understand this, it is instructive to consider <a class="reference external" href="http://gauss.stat.su.se/gu/sg/2012VT/penny.pdf">a paper</a> by Joe Keller, one of the giants of applied mathematics in the <span class="math notranslate nohighlight">\(20^{\text{th}}\)</span> century.  In this paper, Keller treats the coin as a regular Newtonian object, subject to the force of gravity.  He notes that tossing a coin is a relatively simple mechanics problem: it has some initial conditions (at <span class="math notranslate nohighlight">\(t=0\)</span>, the coin is given some rotational and translational inertia), and some governing equations of motion (Newton’s laws).  Given these initial conditions and equations of motion, you can relatively simply have your computer calculate the consequences: starting with <span class="math notranslate nohighlight">\(u\)</span> angular velocity and <span class="math notranslate nohighlight">\(\omega\)</span> translational velocity, the coin will come up heads or tails.  If you change the initial conditions, the outcome will change correspondingly.  (In his analysis, he ignores air resistance, bouncing, and the option to land on the coin’s side, which actually makes his point more salient.)  So then how is it that this deterministic process is our base model for a random process?</p>
<figure class="align-default" id="fig-keller">
<img alt="Figure 2 from *The Probability of Heads* by Keller (1986)." src="../../_images/keller.jpg" />
<figcaption>
<p><span class="caption-number">Fig. 1 </span><span class="caption-text">From <em>The Probability of Heads</em> by Keller (1986).</span><a class="headerlink" href="#fig-keller" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>What Keller shows is that while a coin flip is an entirely deterministic process, the outcome is uncertain due to an inability of any human (or machine!) to prescribe the initial conditions in such a way as to reliably get a heads or tails.  <a class="reference internal" href="#fig-keller"><span class="std std-numref">Fig. 1</span></a> shows the outcome of a coin toss for an initial angular velocity, <span class="math notranslate nohighlight">\(\omega\)</span>, and vertical velocity, <span class="math notranslate nohighlight">\(u\)</span>.  The black lines denote the edges of regions of this parameter space that give rise to heads or tails.  It’s not surprising that these bands alternate between heads and tails, but what is surprising is that the bands alternate increasingly rapidly as <span class="math notranslate nohighlight">\(\omega\)</span> and <span class="math notranslate nohighlight">\(u\)</span> increase.  The bands become increasingly close together so that even the tiniest change in the spin or velocity of the coin will change it from a heads to a tails.  This is an example of the concept of <em>chaos</em>, which is when the outcome of a system is enormously sensitive to initial conditions.  (This is also known as the Butterfly Effect.)  Bringing up this analysis serves to underline that even the most simple systems may need to be considered probabilistically, even if for no reason other than that the mechanistic description is less useful.</p>
</section>
</section>
<section id="flipping-several-coins">
<h3>Flipping Several Coins<a class="headerlink" href="#flipping-several-coins" title="Permalink to this headline">#</a></h3>
<p>Accepting then that a coin is a random process, it may not yet be clear how we can learn much from flipping one coin at a time.  In fact, one coin is not terribly exciting, but if we flip, say, <span class="math notranslate nohighlight">\(N\)</span> coins, things become much more interesting.  Most obviously, flipping <span class="math notranslate nohighlight">\(N\)</span> coins has many more possible outcomes, from <span class="math notranslate nohighlight">\(N\)</span> heads to <span class="math notranslate nohighlight">\(N\)</span> tails and everything in between.  If we want to write down the probability of observing say, 8 heads out of 20 coin tosses, we can take the likelihood of getting 8 heads and 12 tails: <span class="math notranslate nohighlight">\(p^8(1-p)^{12}\)</span>. (Why does this formula make sense?)  However, there are many ways to get 8 H and 12 T, the first 8 flips can all be heads and the last 12 can all be tails, or the first 7 can be heads and the next 12 tails and the last a heads again, etc.  To account for this we use the combinatorial factor <span class="math notranslate nohighlight">\({20 \choose 8} = \frac{20!}{8!12!}\)</span>, where the “<code class="docutils literal notranslate"><span class="pre">!</span></code>” is the <strong>factorial</strong> operator (<span class="math notranslate nohighlight">\(5!=5\cdot4\cdot3\cdot2\cdot1\)</span>, for example).</p>
<p>Writing the above more generally, we get</p>
<div class="math notranslate nohighlight" id="equation-eqn-binomeqn">
<span class="eqno">(3)<a class="headerlink" href="#equation-eqn-binomeqn" title="Permalink to this equation">#</a></span>\[P(k\text{heads in}N\text{flips of a coin}) =
P(k|N, p) =
\frac{N!}{k!(N-k)!}p^k(1-p)^{N-k},\]</div>
<p>which you may recognize as the <strong>binomial distribution</strong>.  We have not yet discussed distributions, but they are the central concept to this entire text.  Most broadly, a probability distribution or <strong>Probability Density Function</strong> (PDF) can be understood as a description of how specific outcomes of a random variable are <em>distributed</em> (how the <em>density</em> of probability is spread out).  So in the case of a single coin with <span class="math notranslate nohighlight">\(P(X=H)=p\)</span>, the PDF is written</p>
<div class="math notranslate nohighlight">
\[\begin{split}P(X=x) = \begin{cases}
    p\qquad&amp;x=H\\
    1-p\qquad&amp;x=T
\end{cases}\end{split}\]</div>
<p>to explicitly state the coin flip distribution.  The binomial distribution then describes how the different number of heads in <span class="math notranslate nohighlight">\(N\)</span> coin flips are distributed.</p>
<figure class="align-default" id="fig-twoempbinom">
<img alt="Visualization of two empirical distributions generated from flipping 10 fair coins 100 times.  The theoretical binomial distribution is plotted over each in orange." src="../../_images/TwoEmpDists_Binom_Chapter2Figure.jpg" />
<figcaption>
<p><span class="caption-number">Fig. 2 </span><span class="caption-text">Visualization of two empirical distributions generated from flipping 10 fair coins 100 times.  The theoretical binomial distribution is plotted over each in orange.</span><a class="headerlink" href="#fig-twoempbinom" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<!-- \begin{figure}
\centering
\includegraphics[width=\linewidth]{TwoEmpDists_Binom_Chapter2Figure.pdf}
\caption{Visualization of two empirical distributions generated from flipping 10 fair coins 100 times.  The theoretical binomial distribution is plotted over each in orange.}
\label{fig:TwoEmpBinom}
\end{figure} -->
<p>Probability distributions, loosely speaking, come in two flavors: empirical and theoretical.  Empirical PDFs (ePDFs) are of the kind that you’ll generate in the worksheets and problem sets; generated from a finite amount of data and proportional to a frequency distribution.  Theoretical probability distributions instead (usually) have a closed mathematical formula, and can be thought to represent the limiting empirical distribution in the case where you have infinite data.  An example of each can be found in <a class="reference internal" href="#fig-twoempbinom"><span class="std std-numref">Fig. 2</span></a> where two different instances of an empirical distribution are shown as the blue bars and a theoretical PDF is shown by the orange line in each panel.  As an analogy, the difference between empirical and theoretical probability distributions is similar to the difference between the equation of a circle, <span class="math notranslate nohighlight">\(x^2+y^2=1\)</span>, and any real world drawing of a circle.  The real world drawing approximates the circle, and there are ways to improve this approximation, but the equation is always a Platonic ideal that cannot be achieved due to the finite nature of the physical universe.  Similarly, repeated constructions of an empirical probability distribution will result in slightly different approximations to the ideal, as can be seen in <a class="reference internal" href="#fig-twoempbinom"><span class="std std-numref">Fig. 2</span></a>.</p>
<p>While this may sound somewhat philosophical, it is crucial to understand that any real world measurement is an empirically acquired outcome of some random process, and therefore will change upon repeated observation.  Hopefully you can see that while we would ideally like to make many observations, so as to put ourselves closer to the regime of the theoretical distribution, this is obviously not always the case.  In fact, it is relatively common to find ourselves in the data-poor limit of an experiment, so it’s very important to be especially cognizant of the fact that we have empirical observations.  As an example, if you toss a fair coin 10 times, it is very possible to get 8 heads (this should happen <span class="math notranslate nohighlight">\(\sim5\%\)</span> of the time according to Equation <a class="reference internal" href="#equation-eqn-binomeqn">(3)</a>!).  If you were to then assess that the coin is biased, you would be wrong!  The goal of of probability and statistics is to help you separate what is signal from what is noise by allowing you to assess the likelihoods of different outcomes.</p>
</section>
<section id="features-of-distributions">
<h3>Features of Distributions<a class="headerlink" href="#features-of-distributions" title="Permalink to this headline">#</a></h3>
<p>One of the most fundamental insights in all of science is that while the outcome of a single experiment, say a single coin toss, is uncertain, there is a regularity to the outcome of many experiments.  You will see this play out in the worksheets and assignments, but you should also recognize this from your own life; if you were to measure a table’s length with a ruler and get 1.8 meters, and then you measured it again, you would not get 12 meters.  You may however get 1.81 or 1.79 meters depending on how you angled your ruler.  If you made many measurements of the table’s length, you would probably get a very narrow distribution of table lengths centered at 1.8 meters, and you would be very justified in reporting that the table is 1.8 meters long.</p>
<p>This may seem somewhat trivial, but the point is this: we’ve elaborated that measurements are random events where the likelihood of any outcome has some (unknown) distribution, and we’ve noted that repeated observations allow us to get closer to a theoretical distribution, so when we are taking measurements of some quantity, we’re not trying to get a better list of numbers, we’re trying to <em>discover the underlying distribution</em> that governs how a quantity is observed.  That is, we want to know about a quantity’s probability distribution in order to characterize it.  Furthermore, we can assert that all quantities are actually distributions from which we draw observations.</p>
<p>While this may seem somewhat abstract - and it is - what it means to you as a quantitative thinker is that we should be very concerned with the <em>shape</em> of our observations’ probability distribution.  It may not be clear why we can expect our observations to have any regularity, and we’ll get to that shortly, but for now, let’s discuss what sort of features of our distributions are interesting.</p>
<section id="moments-of-distributions">
<h4>Moments of Distributions<a class="headerlink" href="#moments-of-distributions" title="Permalink to this headline">#</a></h4>
<p>Perhaps most obviously, we might be interested in the most frequent outcome of a random event (this is known as the <strong>mode</strong> of the distribution).  For example, if you were given a coin with an unknown bias, you can imagine conducting many experiments of many coin flips and looking at what the most frequent result was to determine whether the coin is fair or not.  That is, we can potentially (hopefully) infer the fairness of the coin by observing properties of its probability distribution.</p>
<p>However, as we’ll show in assignments and in later sections, looking at peaks of distributions is not the only useful feature.  For example, consider <a class="reference internal" href="#fig-twoempbinom"><span class="std std-numref">Fig. 2</span></a>, in which the distributions for the number of heads in 10 coin flips are shown after performing the experiment (10 coin flips) 100 times.  According to the left panel, the mode is 5 heads (in 10 flips), and according to the right panel, the mode is 4 flips.  If we had only this information, it would be very hard to know whether the same coin was used to generate the distributions (it was!).</p>
<p>In fact, there are some general mathematical ways to quantify different features of a distribution.  In particular, we can talk about the <strong>mean</strong> or <strong>expectation</strong> of a distribution, which can be calculated either from a theoretical probability distribution</p>
<div class="math notranslate nohighlight">
\[E[X] = \int_{x\in X}xP(X=x)dx,\]</div>
<p>or from an empirical distribution</p>
<div class="math notranslate nohighlight" id="equation-eqn-expval">
<span class="eqno">(4)<a class="headerlink" href="#equation-eqn-expval" title="Permalink to this equation">#</a></span>\[E[X] = \sum_{x\in X}xP(X=x),\]</div>
<p>where <span class="math notranslate nohighlight">\(\Sigma\)</span> is the summation operator, so we want to add up <span class="math notranslate nohighlight">\(x\)</span> times <span class="math notranslate nohighlight">\(P(X=x)\)</span> for all possible values of <span class="math notranslate nohighlight">\(x\)</span>.  So for example, if we consider <span class="math notranslate nohighlight">\(X\)</span> to be the roll of a fair die, then <span class="math notranslate nohighlight">\(x\)</span> can be either <span class="math notranslate nohighlight">\(1, 2, 3, 4, 5, \)</span> or <span class="math notranslate nohighlight">\(6\)</span> and the full sum can be written</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    E[X] &amp;=  (1)\cdot P(X=1) + (2)\cdot P(X=2) + (3)\cdot P(X=3)\\
    &amp;\qquad\qquad + (4)\cdot P(X=4) + (5)\cdot P(X=5) + (6)\cdot P(X=6) \\
    &amp;= 1\cdot\frac{1}{6} + 2\cdot\frac{1}{6} + 3\cdot\frac{1}{6} + 4\cdot\frac{1}{6} + 5\cdot\frac{1}{6} + 6\cdot\frac{1}{6} = 3.5, 
\end{align*}\]</div>
<p>and we expect the value of the die to yield 3.5 on average.  (Make sure this makes sense to you!)  Note that we talk about expected values of <em>random variables</em>, <span class="math notranslate nohighlight">\(X\)</span>, not particular outcomes (<span class="math notranslate nohighlight">\(x=1\)</span>, for example); the outcome of a random variable is uncertain, but if the die came up 1, then that’s our value.</p>
<p>If we think of the mean as a measurement of a distribution’s <em>center</em>, how can we describe its width?  One measure of “width” is the <strong>variance</strong>, which can be calculated</p>
<div class="math notranslate nohighlight" id="equation-eqn-var">
<span class="eqno">(5)<a class="headerlink" href="#equation-eqn-var" title="Permalink to this equation">#</a></span>\[Var[X] = E[(X-\mu)^2] = E[X^2] - E[X]^2,\]</div>
<p>where <span class="math notranslate nohighlight">\(\mu = E[X]\)</span> and <span class="math notranslate nohighlight">\(E[X^2] = \sum_{x\in X}x^2P(X=x)\)</span>.  Using this formula, we find that the variance of a single die roll is</p>
<div class="math notranslate nohighlight">
\[Var[X] =  1\cdot\frac{1}{6} + 4\cdot\frac{1}{6} + 9\cdot\frac{1}{6} + 16\cdot\frac{1}{6} + 25\cdot\frac{1}{6} + 36\cdot\frac{1}{6} - (3.5)^2 = \frac{91}{6} - (3.5)^2 = 2.9167.\]</div>
<p>The <strong>standard deviation</strong> of a set of observations is another common measure of spread and is defined as the square root of the variance (<span class="math notranslate nohighlight">\(\sigma = \sqrt{Var[X]}\)</span>).</p>
<p>You can imagine that there are a whole set of measurements <span class="math notranslate nohighlight">\(E[(X-\mu)^n]\)</span>, and indeed, these are called the <strong>moments</strong> of a distribution (<span class="math notranslate nohighlight">\(E[(X-\mu)^n]\)</span> is the <span class="math notranslate nohighlight">\(n^{\text{th}}\)</span> moment).  In fact, as <span class="math notranslate nohighlight">\(n\)</span> increases, you learn about the shape of the distribution further and further from the mean.  Specifically, the odd values of <span class="math notranslate nohighlight">\(n\)</span> give information about how asymmetric the distribution is and the even values about how symmetric the distribution is.  A certain version of <span class="math notranslate nohighlight">\(n=3\)</span> is called the <strong>skewness</strong> and <span class="math notranslate nohighlight">\(n=4\)</span> is known as the <strong>kurtosis</strong> of the distribution.</p>
<div class="admonition-try-it-yourself admonition">
<p class="admonition-title">Try It Yourself</p>
<p>To put this theory into practice, take a stab at <a class="reference internal" href="Worksheet_1_1_CoinFlipping.html"><span class="doc std std-doc">Worksheet 1.1</span></a>.</p>
<p>Once you have completed this worksheet, you should have enough practice to start to work on <a class="reference internal" href="Assignment_1.html"><span class="doc std std-doc">Assignment 1</span></a>!</p>
</div>
</section>
<section id="cumulative-distribution-functions">
<h4>Cumulative Distribution Functions<a class="headerlink" href="#cumulative-distribution-functions" title="Permalink to this headline">#</a></h4>
<p>One important note to make here is that when we have empirical distributions, we need to be careful about how we define <span class="math notranslate nohighlight">\(P(X)\)</span>.  Say for example, you were measuring the heights of everyone at Northwestern, but rather than keeping track of thousands of heights, you were just counting how many people’s heights fell into a set of bins, e.g. 120cm-130cm, 130cm-140cm, etc.  (Would this be a distribution??)  As you’ll explore in worksheets and assignments, your choices as to bin size and number will affect any calculations of moments that you make later on.</p>
<div class="admonition-try-it-yourself admonition">
<p class="admonition-title">Try It Yourself</p>
<p>Change the bin sizes in your histograms and distributions in <a class="reference internal" href="Worksheet_1_1_CoinFlipping.html"><span class="doc std std-doc">Worksheet 1.1</span></a>.  That is, instead of showing how many times you observed 5 heads, 6 heads, 7 heads, etc.  Show how many times you observed 0, 1, or 2 heads, the number of times you observed 3, 4, and 5 heads, etc.  How do any calculations based on the PDF change?  Try and modify any curves generated using *theory} to use the same bins.</p>
</div>
<p>While this is not catastrophic, it’s also not desirable, so it is worth noting that we can also characterize the shape of a distribution using <strong>percentiles</strong>.  A percentile is defined as the value of an observation such that a certain percent of all the observations are less than or equal to that value.  More formally:</p>
<div class="math notranslate nohighlight">
\[Q_X(p) = \min_{x\in X}\left\{x \left| \sum_{\chi \leq x}P(X=\chi) \geq p\right. \right\}\]</div>
<p>To calculate percentiles empirically, we will want to generate a <strong>cumulative distribution function</strong>, or CDF (we’ll sometimes denote an empirical CDF as eCDF to distinguish from theoretically derived CDFs):</p>
<div class="math notranslate nohighlight">
\[F[x] = \int_{-\infty}^xP(X=x)dx.\]</div>
<p>In words, the CDF describes the likelihood of getting an outcome less than <span class="math notranslate nohighlight">\(x\)</span>, and as such is also sometimes written <span class="math notranslate nohighlight">\(P(X\leq x)\)</span>.  We can do this empirically using the following Python code:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">collections</span>

<span class="c1">## Counts the number of times each value in data is seen </span>
<span class="n">counts</span> <span class="o">=</span> <span class="n">collections</span><span class="o">.</span><span class="n">Counter</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>  
                                
<span class="c1">## Gets unique values of data and sorts</span>
<span class="n">vals</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">msort</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">counts</span><span class="o">.</span><span class="n">keys</span><span class="p">()))</span>    

<span class="c1">## List comprehension to get ordered counts for each value</span>
<span class="c1">## cumsum then cumulatively sums these counts</span>
<span class="n">CDF</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">counts</span><span class="p">[</span><span class="n">ii</span><span class="p">]</span> <span class="k">for</span> <span class="n">ii</span> <span class="ow">in</span> <span class="n">vals</span><span class="p">]))</span>

<span class="c1">## Normalize so that the PDF adds to 1.</span>
<span class="n">CDF</span> <span class="o">=</span> <span class="n">CDF</span> <span class="o">/</span> <span class="n">CDF</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
</pre></div>
</div>
<p>Using this code, we can generate CDFs for the data shown in Figure <a class="reference internal" href="#fig-twoempbinom"><span class="std std-numref">Fig. 2</span></a>, which are shown in <a class="reference internal" href="#fig-twoempcdfsbinom"><span class="std std-numref">Fig. 3</span></a>.  Then, using the CDFs, we can easily read off percentiles by finding the desired percentile on the <span class="math notranslate nohighlight">\(y\)</span>-axis and finding the corresponding <span class="math notranslate nohighlight">\(x\)</span>-coordinate on the CDF.  For example, in Figure \ref{fig:TwoEmpCDFsBinom}, we can see that the 20th percentile of both distributions is approximately <span class="math notranslate nohighlight">\(k=3\)</span> heads.</p>
<div class="admonition-try-it-yourself admonition">
<p class="admonition-title">Try It Yourself</p>
<p>Use the above code to make CDFs of the distributions in <a class="reference internal" href="Worksheet_1_1_CoinFlipping.html"><span class="doc std std-doc">Worksheet 1.1</span></a>.</p>
</div>
<p>Particularly interesting quantities are the <strong>median</strong>, which is the 50th percentile, and the <strong>Inter-Quartile Range</strong> (IQR), which is the distance between the 25th and 75th percentiles (the first and third <strong>quartiles</strong>).  In both the examples in <a class="reference internal" href="#fig-twoempcdfsbinom"><span class="std std-numref">Fig. 3</span></a>, the median and IQR are 5 and 2, respectively.  The median and IQR are often more useful for characterizing a distribution than the mean and variance because they are <em>robust to outliers</em> in your observations.</p>
<figure class="align-default" id="fig-twoempcdfsbinom">
<img alt="../../_images/TwoEmpDists_Binom_Chapter2Figure.jpg" src="../../_images/TwoEmpDists_Binom_Chapter2Figure.jpg" />
<figcaption>
<p><span class="caption-number">Fig. 3 </span><span class="caption-text">Empirical and theoretical CDFs for the number of heads in 100 experiments of 10 coin tosses.  Note that in neither of the 100 experiments was 0 heads observed, so neither empirical CDF extends to <span class="math notranslate nohighlight">\(x=0\)</span>.  Similarly, in the left panel’s 100 experiments, no observations of 9 or 10 heads were made either.  The median, 25th, and 75th percentiles are also shown, and the IQR is indicated.</span><a class="headerlink" href="#fig-twoempcdfsbinom" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<!-- \begin{figure}[htbp]
\centering
\captionsetup{width=0.8\linewidth}
\includegraphics[width=\linewidth]{TwoEmpCDFs_Binom_Chapter2Figure.pdf}
\caption{Empirical and theoretical CDFs for the number of heads in 100 experiments of 10 coin tosses.  Note that in neither of the 100 experiments was 0 heads observed, so neither empirical CDF extends to $x=0$.  Similarly, in the left panel's 100 experiments, no observations of 9 or 10 heads were made either.  The median, 25th, and 75th percentiles are also shown, and the IQR is indicated.  FIX FIGURE TO NOT INTERPOLATE!!!}
\label{fig:TwoEmpCDFsBinom}
\end{figure} -->
<p>It’s worth mentioning that while the binomial distribution is a useful and intuitive distribution to know, it is certainly not the only theoretical distribution worth knowing about.  In particular, <strong>Normal</strong> or <strong>Gaussian</strong> distributions and <strong>Poisson</strong> distributions are often useful in quantitative analyses.  However, their functional forms are not essential for the understanding of the main part of this chapter, so we will leave them to the end.</p>
<div class="admonition-try-it-yourself admonition">
<p class="admonition-title">Try It Yourself</p>
<p>Use <code class="docutils literal notranslate"><span class="pre">numpy.random.randn</span></code> to generate 2 lists of 10 normally distributed random numbers.  Add the value 100 to one of the lists. Compare the mean and median of the 2 lists.  Which is the better descriptor of the “average” value of the list, the mean or median?</p>
</div>
</section>
</section>
<section id="the-central-limit-theorem">
<h3>The Central Limit Theorem<a class="headerlink" href="#the-central-limit-theorem" title="Permalink to this headline">#</a></h3>
<p>In the previous sections, we’ve mentioned that theoretical distributions are the infinite-data limit of observational (sampling) distributions, but we have not provided any rigorous reason why our approximations get better as we make more observations.  We will not do so here except to highlight the famous Central Limit Theorem (CLT), whose many proofs provide the basis for our assertion.  We will not prove the CLT here, instead we find it most useful just to highlight its main result and assumptions.</p>
<p>Specifically, the CLT says that in an extraordinary number of cases, the ratio of the standard deviation (<span class="math notranslate nohighlight">\(\sigma\)</span>) to the mean (<span class="math notranslate nohighlight">\(\mu\)</span>) of a set of observations decreases as the number of observations, <span class="math notranslate nohighlight">\(N\)</span> is increased.  Moreover, the ratio decreases in a specific fashion: as the square root of <span class="math notranslate nohighlight">\(N\)</span>, as shown below.</p>
<div class="math notranslate nohighlight" id="equation-eqn-clt">
<span class="eqno">(6)<a class="headerlink" href="#equation-eqn-clt" title="Permalink to this equation">#</a></span>\[\frac{\sigma}{\mu}\sim \frac{1}{\sqrt{N}}\]</div>
<p>More intuitively, if we think of the mean as a measure of the “center” of our distribution, and the standard deviation as the error in estimating the true mean, then our ability to estimate the first moment of our distribution increases as the number of data points is increased.</p>
<p>Now you might see how this could be useful.  If we’re concerned about separating signal (<span class="math notranslate nohighlight">\(\mu\)</span>) from noise (<span class="math notranslate nohighlight">\(\sigma\)</span>), then we can use the CLT to determine exactly how many observations we need to estimate the mean to a certain accuracy.  However, the CLT requires a few assumptions about the data in order to make this statement:</p>
<ul class="simple">
<li><p>all measurements are independent of one another, and</p></li>
<li><p>all observations must be generated from the same underlying process.
In terms of a coin toss, as long as each coin being tossed has the same value for <span class="math notranslate nohighlight">\(P(X=H)=p\)</span>, then tossing increasing numbers of coins improves our empirical estimation of <span class="math notranslate nohighlight">\(p\)</span>.  That is, the CLT tells you that if you continue flipping a coin enough times, you can be increasingly confident calculating <span class="math notranslate nohighlight">\(p\)</span>.  Hopefully this is reassuring and intuitive: adding more data should improve our inferences and estimates.</p></li>
</ul>
<div class="admonition-try-it-yourself admonition">
<p class="admonition-title">Try It Yourself</p>
<p>Consider the experiment from <a class="reference internal" href="Worksheet_1_1_CoinFlipping.html"><span class="doc std std-doc">Worksheet 1.1</span></a> where you are flipping <span class="math notranslate nohighlight">\(N\)</span> coins repeatedly.  If I want the standard deviation in the number of heads to be <em>on average</em> 10% of the mean, how many coins do I have to flip?  Show this with code if you can (or if you’re having trouble with the theory)!</p>
</div>
<p>The flipside of this is that when we have very few measurements of a phenomenon, then our ability to estimate a given feature of the distribution, say the mean, will be error-prone.  The CLT tells us that the error in these estimates will often scale with the inverse square root, as shown in Equation <a class="reference internal" href="#equation-eqn-clt">(6)</a>, but that is not much comfort if we only have 2 or 3 measurements.  This is worth keeping in mind whenever you are working with data, even if it is “big”; there is a difference between having a few measurements of many different quantities and many measurements of just one quantity.</p>
<p>If this were all the CLT gave us, it would be a useful result, but the CLT also can be stated (in combination with the Law of Large Numbers) to say that if you have enough measurements (if <span class="math notranslate nohighlight">\(N\)</span> is big) of an independently and identically distributed (i.i.d.) random variable, then the distribution of the mean of those measurements can be characterized completely with only the mean and standard deviation (the first two moments) of the random variable.  For those with some previous experience in these topics, the CLT says that the <em>sum</em> of i.i.d. random variables becomes a Gaussian distribution as the number of observations, <span class="math notranslate nohighlight">\(N\)</span>, increases.  Even more specifically, if the i.i.d. random variables have mean <span class="math notranslate nohighlight">\(\mu\)</span> and standard deviation <span class="math notranslate nohighlight">\(\sigma\)</span>, then the distribution of the mean has mean <span class="math notranslate nohighlight">\(\mu\)</span> and standard deviation <span class="math notranslate nohighlight">\(\sigma/\sqrt{N}\)</span>. This may seem like an odd thing to focus on but it’s actually very useful as we now know how <em>any sum</em> of i.i.d. variables is distributed.  While in most of this course we will stay away from making assumptions as to the form of any distribution, it is quite remarkable that all sums of random variables are distributed similarly.</p>
<p>The upshot of the CLT is this: when the assumptions are met, it is a very useful result!  However, in many situations we do not have many i.i.d. random variables, so it is not prudent to preoccupy oneself only with analyses that rely on the CLT to work.  Instead, we hope to build up some methods that will apply even when the assumptions of the CLT are not strictly valid.</p>
<div class="admonition-try-it-yourself admonition">
<p class="admonition-title">Try It Yourself</p>
<p>Try your hand at <a class="reference internal" href="Worksheet_1_2_SquareRootN.html"><span class="doc std std-doc">Worksheet 1.2</span></a>.  You should also now be able to attack most of <a class="reference internal" href="Assignment_1.html"><span class="doc std std-doc">Assignment 1</span></a>!</p>
</div>
</section>
</section>
<section id="the-atom-of-bayesian-probability">
<h2>The Atom of Bayesian Probability<a class="headerlink" href="#the-atom-of-bayesian-probability" title="Permalink to this headline">#</a></h2>
<p>Throughout this text, our goal is to remove obstructing details and formulas from your path towards becoming a quantitative worker.  While some details are inevitably necessary in any field, what we have been emphasizing are the most important and most useful parts of theory for actually learning things from data.  If you have previously taken a statistics course, it may come as a surprise to you then that we think it’s worthwhile to talk about Bayes’ Theorem.  Our motivation here is partly philosophical, but also (as always) practical.</p>
<p>First, Bayes’ Theorem can be written explicitly:</p>
<div class="math notranslate nohighlight" id="equation-eqn-bayes">
<span class="eqno">(7)<a class="headerlink" href="#equation-eqn-bayes" title="Permalink to this equation">#</a></span>\[P(A | B) = \frac{P(B | A)\cdot P(A)}{P(B)}.\]</div>
<p>As noted earlier, the expression <span class="math notranslate nohighlight">\(P(A | B)\)</span> is read “the probability of random event <span class="math notranslate nohighlight">\(A\)</span> *given} random event <span class="math notranslate nohighlight">\(B\)</span>” and  is called a **conditional probability}.  This name is due to the fact that we are thinking about the likelihoods of outcomes of random event <span class="math notranslate nohighlight">\(A\)</span> <em>conditioned on</em> (depending on) the outcome of random event <span class="math notranslate nohighlight">\(B\)</span>.  In this way, Bayes’ Theorem is simply a formula relating the conditional probabilities of two random events.</p>
<div class="admonition-try-it-yourself admonition">
<p class="admonition-title">Try It Yourself</p>
<p>Using Equation <a class="reference internal" href="#equation-eqn-jointdist">(2)</a> in two symmetric ways, see if you can derive Bayes’ Theorem yourself.</p>
</div>
<p>To make it clear why Bayes’ Theorem might be useful, let’s consider random event <span class="math notranslate nohighlight">\(B\)</span> to be the act of observing data and random event <span class="math notranslate nohighlight">\(A\)</span> to be some interesting feature of the data distribution, let’s say the mean.  Then as a data researcher, we might be concerned with knowing that interesting feature, and if we’re being careful, we might be very concerned with <em>how well</em> we can know that feature <em>given that we collected certain data</em>.  That is, we are almost always concerned with knowing <span class="math notranslate nohighlight">\(P(\text{FEATURE}|\text{DATA})\)</span>, the likelihood that our feature has a specific value given our specific data.  This quantity is known as the <strong>posterior</strong> distribution for the feature <span class="math notranslate nohighlight">\(A\)</span>.</p>
<div class="math notranslate nohighlight" id="equation-eqn-bayesdatafeature">
<span class="eqno">(8)<a class="headerlink" href="#equation-eqn-bayesdatafeature" title="Permalink to this equation">#</a></span>\[P(FEATURE|DATA) = \frac{P(DATA|FEATURE) P(FEATURE)}{P(DATA)}\]</div>
<p>The reason then that Bayes’ Theorem is so appealing is that it gives us an explicit structure in which to calculate this conditional likelihood.  Specifically, if we can construct the parts of the right-hand side of Equation <a class="reference internal" href="#equation-eqn-bayes">(7)</a>, we can plug them into Bayes’ Theorem and we have our answer.  You may be concerned however as to how we might know the parts of the right-hand side better than the left, and this is a main criticism of Bayesian statistics, however Bayesian practitioners prefer to keep their assumptions front and center (as a main part of any calculation), than not. But before we get into the philosophical side, what even is on the right-hand side of this equation?</p>
<p>Keeping the analogy that <span class="math notranslate nohighlight">\(A = \)</span>FEATURE and <span class="math notranslate nohighlight">\(B = \)</span>DATA, then <span class="math notranslate nohighlight">\(P(DATA|FEATURE)\)</span> is the opposite conditional statement to the one we’re interested in, and is called the <strong>likelihood function</strong>.  That is, it is the conditional likelihood of observing our data given some feature of the sampling distribution.  In many cases (we’ll get into this), we have a <strong>model</strong> for how our data are generated, and this model may depend on some <strong>parameters</strong>, which may be features of a distribution.  In any case, when we have such a model, Bayes’ Theorem accounts for it with this term.</p>
<p>The term <span class="math notranslate nohighlight">\(P(A)\)</span> is known as the <strong>prior</strong> and it represents our <em>prior</em>knowledge about how we think that the feature <span class="math notranslate nohighlight">\(A\)</span> is distributed <em>before</em> we collect any data.  You may be inclined to assert that we don’t have any information about <span class="math notranslate nohighlight">\(A\)</span>, otherwise we wouldn’t be doing experiments to measure it, but this isn’t really  true - we often do have some guesses as to the shape or size of <span class="math notranslate nohighlight">\(A\)</span>.  For example, if I hand you a coin and tell you it’s from the U.S. Mint, it would not be entirely reasonable of you to state that you have absolutely no notion as to how often the coin will land on heads.  Based on personal experience alone, it would be more prudent to start from the assumption that the coin is probably fair and wait for new evidence to convince you otherwise.  It’s worth mentioning that even if you want to insist that you don’t know anything, you can often plug in what is called a <strong>non-informative</strong> prior, for example, a uniform distribution for <span class="math notranslate nohighlight">\(p\)</span> from 0 to 1 in the case of the coin toss.  Also, regardless of your choice of prior, given enough evidence Bayes’ Theorem will always converge to the same posterior distribution, so this seemingly arbitrary choice doesn’t often end up having that large of an impact.  (We’ll explore this more concretely in a worksheet later.)</p>
<p>Let’s now work a specific example to see how this works.</p>
<section id="example-bayesian-coin-tossing">
<h3>Example: Bayesian Coin Tossing<a class="headerlink" href="#example-bayesian-coin-tossing" title="Permalink to this headline">#</a></h3>
<p>While we’ll talk about parameter estimation more in the next section, let’s consider the scenario where you have been given a coin and you want to assess whether it is fair.  We’ll use Bayes’ Theorem in a slightly looser formulation:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
P(p | k\text{ Heads in } N\text{ tosses}) &amp;\propto P(k\text{ Heads in } N\text{ tosses}| p) \cdot P(p)\\
P(p| k, N) &amp;\propto P_{Binom}(k|N, p) \cdot P(p),
\end{align*}\]</div>
<p>where we omit the denominator, <span class="math notranslate nohighlight">\(P(k\text{ Heads in } N\text{ tosses})\)</span>, because we know that the left-hand side is a probability distribution and thus must sum to 1.  Let’s then say that <span class="math notranslate nohighlight">\(P(p)\)</span>, our prior for the distribution of the heads probability, <span class="math notranslate nohighlight">\(p\)</span>, is a uniform distribution from 0 to 1 (often denoted <span class="math notranslate nohighlight">\(\mathcal{U}(0, 1)\)</span>).  That is, we’re asserting that we really don’t know anything about coins.</p>
<p>We know from our earlier derivation of the binomial distribution, that if we know <span class="math notranslate nohighlight">\(p\)</span>, then we can write the likelihood of observing any number of heads <span class="math notranslate nohighlight">\(k\)</span> in <span class="math notranslate nohighlight">\(N\)</span> tosses is binomially distributed, so we write</p>
<div class="math notranslate nohighlight">
\[P(k\text{ Heads in } N\text{ tosses}| p) = P(k|N, p) =  {N\choose k}p^k(1-p)^{N-k},\]</div>
<p>which is the first factor on the right-hand side of Bayes’ Theorem.</p>
<p>Then, before we start flipping the coin, we can visualize our prior for the heads probability in <a class="reference internal" href="#fig-bayesexample-0-noinfo"><span class="std std-numref">Fig. 4</span></a>.  As a counter-point, we also consider a prior in which we strongly believe that the coin will be fair, with much less likelihood that it is biased.  This is shown in <a class="reference internal" href="#fig-bayesexample-0-info"><span class="std std-numref">Fig. 5</span></a>.  In this thought experiment, the coin actually is biased to have a heads probability of <span class="math notranslate nohighlight">\(p=0.28,\)</span> which is shown in the figures, but of course, we wouldn’t know this when we’re given the coin.</p>
<figure class="align-default" id="fig-bayesexample-0-noinfo">
<a class="reference internal image-reference" href="../../_images/BayesExample_toss0_nonInfoPrior.jpg"><img alt="../../_images/BayesExample_toss0_nonInfoPrior.jpg" src="../../_images/BayesExample_toss0_nonInfoPrior.jpg" style="width: 345.6px; height: 345.6px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 4 </span><span class="caption-text">A non-informative prior (uniform distribution).</span><a class="headerlink" href="#fig-bayesexample-0-noinfo" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<figure class="align-default" id="fig-bayesexample-0-info">
<a class="reference internal image-reference" href="../../_images/BayesExample_toss0_infoPrior.jpg"><img alt="../../_images/BayesExample_toss0_infoPrior.jpg" src="../../_images/BayesExample_toss0_infoPrior.jpg" style="width: 345.6px; height: 345.6px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 5 </span><span class="caption-text">An informative prior (non-uniform distribution).</span><a class="headerlink" href="#fig-bayesexample-0-info" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<!-- \noindent
\begin{minipage}[htbp]{0.5\linewidth}
\centering
\includegraphics[width=.95\linewidth]{BayesExample_toss0_nonInfoPrior.pdf}
\captionsetup{width=.95\linewidth}
\captionof{figure}{A non-informative prior (uniform distribution) for the heads probability of a coin.}
\label{fig:BayesExample_0_NoInfo}
\end{minipage}
\begin{minipage}{.5\linewidth}
\centering
\includegraphics[width=.95\linewidth]{BayesExample_toss0_infoPrior.pdf}
\captionsetup{width=.95\linewidth}
\captionof{figure}{An informative prior (non-uniform distribution) for the heads probability of a coin.}
\label{fig:BayesExample_0_Info}
\end{minipage} -->
<p>In <a class="reference internal" href="#fig-bayesexample-1-noinfo"><span class="std std-numref">Fig. 6</span></a> and <a class="reference internal" href="#fig-bayesexample-1-info"><span class="std std-numref">Fig. 7</span></a>, we see the result of multiplying <span class="math notranslate nohighlight">\(P(0 H | N=1, p)\)</span> and our two priors <span class="math notranslate nohighlight">\(P(p)\)</span>.  That is, we flipped the coin and got a tails, so we shifted our estimates of the likelihood of heads <em>away</em> from <span class="math notranslate nohighlight">\(p=1\)</span> and towards <span class="math notranslate nohighlight">\(p=0\)</span>, although in the case of our informed prior we’re still holding out considerably for a fair coin.</p>
<p>That is, we’re showing the function given by</p>
<div class="math notranslate nohighlight">
\[f(p) = P_{Binom}(k{=}0|N{=}1, p) \times P(p),\]</div>
<p>for two different choices of <span class="math notranslate nohighlight">\(P(p)\)</span> (as shown in <a class="reference internal" href="#fig-bayesexample-0-noinfo"><span class="std std-numref">Fig. 4</span></a> and <a class="reference internal" href="#fig-bayesexample-0-info"><span class="std std-numref">Fig. 5</span></a>).</p>
<figure class="align-default" id="fig-bayesexample-1-noinfo">
<a class="reference internal image-reference" href="../../_images/BayesExample_toss1_nonInfoPrior.jpg"><img alt="../../_images/BayesExample_toss1_nonInfoPrior.jpg" src="../../_images/BayesExample_toss1_nonInfoPrior.jpg" style="width: 345.6px; height: 345.6px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 6 </span><span class="caption-text">A posterior distribution for the heads probability of a coin after one toss with a non-informative prior.</span><a class="headerlink" href="#fig-bayesexample-1-noinfo" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<figure class="align-default" id="fig-bayesexample-1-info">
<a class="reference internal image-reference" href="../../_images/BayesExample_toss1_infoPrior.jpg"><img alt="../../_images/BayesExample_toss1_infoPrior.jpg" src="../../_images/BayesExample_toss1_infoPrior.jpg" style="width: 345.6px; height: 345.6px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 7 </span><span class="caption-text">A posterior distribution for the heads probability of a coin after one toss with an informative prior.</span><a class="headerlink" href="#fig-bayesexample-1-info" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<!-- \noindent
\begin{minipage}[htbp]{0.5\linewidth}
\centering
\includegraphics[width=.95\linewidth]{BayesExample_toss1_nonInfoPrior.pdf}
\captionsetup{width=.95\linewidth}
\captionof{figure}{A posterior distribution for the heads probability of a coin after one toss with a non-informative prior.}
\label{fig:BayesExample_1_NoInfo}
\end{minipage}
\begin{minipage}{.5\linewidth}
\centering
\includegraphics[width=.95\linewidth]{BayesExample_toss1_infoPrior.pdf}
\captionsetup{width=.95\linewidth}
\captionof{figure}{A posterior distribution for the heads probability of a coin after one toss with an informative prior.}
\label{fig:BayesExample_1_Info}
\end{minipage} -->
<p>In <a class="reference internal" href="#fig-bayesexample-2-noinfo"><span class="std std-numref">Fig. 8</span></a> and <a class="reference internal" href="#fig-bayesexample-2-info"><span class="std std-numref">Fig. 9</span></a>, we’ve flipped two coins, one of which is a heads, so the posteriors have recentered themselves on <span class="math notranslate nohighlight">\(p=0.5\)</span>. Then in <a class="reference internal" href="#fig-bayesexample-10-noinfo"><span class="std std-numref">Fig. 10</span></a> and <a class="reference internal" href="#fig-bayesexample-10-info"><span class="std std-numref">Fig. 11</span></a>, we’ve skipped ahead to flipping 10 coins, of which 9 were tails!  Now we can see that both posteriors are starting to look similar, and the effect of our prior choice is becoming much less signficant.</p>
<figure class="align-default" id="fig-bayesexample-2-noinfo">
<a class="reference internal image-reference" href="../../_images/BayesExample_toss2_nonInfoPrior.jpg"><img alt="../../_images/BayesExample_toss2_nonInfoPrior.jpg" src="../../_images/BayesExample_toss2_nonInfoPrior.jpg" style="width: 345.6px; height: 345.6px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 8 </span><span class="caption-text">A posterior distribution for the heads probability of a coin after two tosses with a non-informative prior.</span><a class="headerlink" href="#fig-bayesexample-2-noinfo" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<figure class="align-default" id="fig-bayesexample-2-info">
<a class="reference internal image-reference" href="../../_images/BayesExample_toss2_infoPrior.jpg"><img alt="../../_images/BayesExample_toss2_infoPrior.jpg" src="../../_images/BayesExample_toss2_infoPrior.jpg" style="width: 345.6px; height: 345.6px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 9 </span><span class="caption-text">A posterior distribution for the heads probability of a coin after two tosses with an informative prior.</span><a class="headerlink" href="#fig-bayesexample-2-info" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<!-- \noindent
\begin{minipage}[htbp]{0.5\linewidth}
\centering
\includegraphics[width=.95\linewidth]{BayesExample_toss2_nonInfoPrior.pdf}
\captionsetup{width=.95\linewidth}
\captionof{figure}{A posterior distribution for the heads probability of a coin after two tosses with a non-informative prior.}
\label{fig:BayesExample_2_NoInfo}
\end{minipage}
\begin{minipage}{.5\linewidth}
\centering
\includegraphics[width=.95\linewidth]{BayesExample_toss2_infoPrior.pdf}
\captionsetup{width=.95\linewidth}
\captionof{figure}{A posterior distribution for the heads probability of a coin after two tosses with an informative prior.}
\label{fig:BayesExample_2_Info}
\end{minipage} -->
<figure class="align-default" id="fig-bayesexample-10-noinfo">
<a class="reference internal image-reference" href="../../_images/BayesExample_toss10_nonInfoPrior.jpg"><img alt="../../_images/BayesExample_toss10_nonInfoPrior.jpg" src="../../_images/BayesExample_toss10_nonInfoPrior.jpg" style="width: 345.6px; height: 345.6px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 10 </span><span class="caption-text">A posterior distribution for the heads probability of a coin after ten tosses with a non-informative prior.</span><a class="headerlink" href="#fig-bayesexample-10-noinfo" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<figure class="align-default" id="fig-bayesexample-10-info">
<a class="reference internal image-reference" href="../../_images/BayesExample_toss10_infoPrior.jpg"><img alt="../../_images/BayesExample_toss10_infoPrior.jpg" src="../../_images/BayesExample_toss10_infoPrior.jpg" style="width: 345.6px; height: 345.6px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 11 </span><span class="caption-text">A posterior distribution for the heads probability of a coin after ten tosses with an informative prior.</span><a class="headerlink" href="#fig-bayesexample-10-info" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<figure class="align-default" id="fig-bayesexample-100-noinfo">
<a class="reference internal image-reference" href="../../_images/BayesExample_toss100_nonInfoPrior.jpg"><img alt="../../_images/BayesExample_toss100_nonInfoPrior.jpg" src="../../_images/BayesExample_toss100_nonInfoPrior.jpg" style="width: 345.6px; height: 345.6px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 12 </span><span class="caption-text">A posterior distribution for the heads probability of a coin after one hundred tosses with a non-informative prior.</span><a class="headerlink" href="#fig-bayesexample-100-noinfo" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<figure class="align-default" id="fig-bayesexample-100-info">
<a class="reference internal image-reference" href="../../_images/BayesExample_toss100_infoPrior.jpg"><img alt="../../_images/BayesExample_toss100_infoPrior.jpg" src="../../_images/BayesExample_toss100_infoPrior.jpg" style="width: 345.6px; height: 345.6px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 13 </span><span class="caption-text">A posterior distribution for the heads probability of a coin after one hundred tosses with an informative prior.</span><a class="headerlink" href="#fig-bayesexample-100-info" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<!-- \noindent
\begin{minipage}[htbp]{0.5\linewidth}
\centering
\includegraphics[width=.95\linewidth]{BayesExample_toss10_nonInfoPrior.pdf}
\captionsetup{width=.95\linewidth}
\captionof{figure}{A posterior distribution for the heads probability of a coin after ten tosses with a non-informative prior.}
\label{fig:BayesExample_10_NoInfo}
\end{minipage}
\begin{minipage}{.5\linewidth}
\centering
\includegraphics[width=.95\linewidth]{BayesExample_toss10_infoPrior.pdf}
\captionsetup{width=.95\linewidth}
\captionof{figure}{A posterior distribution for the heads probability of a coin after ten tosses with an informative prior.}
\label{fig:BayesExample_10_Info}
\end{minipage}


\noindent
\begin{minipage}[htbp]{0.5\linewidth}
\centering
\includegraphics[width=.95\linewidth]{BayesExample_toss100_nonInfoPrior.pdf}
\captionsetup{width=.95\linewidth}
\captionof{figure}{A posterior distribution for the heads probability of a coin after 100 tosses with a non-informative prior.}
\label{fig:BayesExample_100_NoInfo}
\end{minipage}
\begin{minipage}{.5\linewidth}
\centering
\includegraphics[width=.95\linewidth]{BayesExample_toss100_infoPrior.pdf}
\captionsetup{width=.95\linewidth}
\captionof{figure}{A posterior distribution for the heads probability of a coin after 100 tosses with an informative prior.}
\label{fig:BayesExample_100_Info}
\end{minipage} -->
<p>Finally,  in <a class="reference internal" href="#fig-bayesexample-100-noinfo"><span class="std std-numref">Fig. 12</span></a> and <a class="reference internal" href="#fig-bayesexample-100-info"><span class="std std-numref">Fig. 13</span></a>, we have flipped the coin 100 times, so that any prior expectation for the heads probability is completely wiped out.  Both posteriors are (very reasonably) centered on <span class="math notranslate nohighlight">\(p=.19\)</span> since we’ve observed 19/100 coins to be heads, but the true value of <span class="math notranslate nohighlight">\(p=0.28\)</span> still has a non-zero probability associated with it.</p>
<p>More explicitly, we’re showing the functions</p>
<div class="math notranslate nohighlight">
\[f(p) = P_{Binom}(k{=}1|N{=}2, p) \times P(p),\]</div>
<div class="math notranslate nohighlight">
\[f(p) = P_{Binom}(k{=}1|N{=}10, p) \times P(p),\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[f(p) = P_{Binom}(k{=}19|N{=}100, p) \times P(p).\]</div>
<p>Notice that each of these is a function of our parameter of interest, <span class="math notranslate nohighlight">\(p\)</span>, only.</p>
<div class="admonition-try-it-yourself admonition">
<p class="admonition-title">Try It Yourself</p>
<p>You now have the theoretical basis to make an attempt at <a class="reference internal" href="Worksheet_1_3_EffectOfPriors.html"><span class="doc std std-doc">Worksheet 1.3</span></a>.  Completing the worksheet will likely be necessary before attempting problem (3.f) on the assignment, so make sure you try the whole worksheet!</p>
</div>
</section>
<section id="why-bayes">
<h3>Why Bayes?<a class="headerlink" href="#why-bayes" title="Permalink to this headline">#</a></h3>
<p>At the beginning of the section we described Bayes’ Theorem and how it might be conceptually useful, but the example in above also demonstrates a practical aspect of why Bayes’ Theorem is good to know: it gives us <em>distributions</em> for any features or parameters.  We’ve spent most of this chapter discussing why individual observations should be considered probabilistically, meaning that they are samples from an underlying distribution, so a methodology that naturally is centered on this distributional way of thinking should be appealing.</p>
<p>We won’t probe all the nuances and uses of Bayes’ Theorem in this chapter, but we will make a point of showing how this relatively simple formula can be extended to most of the applications in this course.  For now, we hope that you can start to see how Bayesianism dovetails nicely with a distributional mindset with regards to data; later we’ll show that Bayes’ Theorem is applicable very generally, even in cases where classical statistics breaks down.</p>
</section>
</section>
<section id="bacterial-chemotaxis">
<h2>Bacterial Chemotaxis<a class="headerlink" href="#bacterial-chemotaxis" title="Permalink to this headline">#</a></h2>
<p>The first biological dataset you will be exploring is related to bacterial chemotaxis. There are beautiful texts (<a class="reference external" href="https://books.google.com/books/about/Random_Walks_in_Biology.html?id=DjdgXGLoJY8C">1</a>, <a class="reference external" href="https://books.google.com/books/about/Biological_Physics_Updated_Edition.html">2</a>, <a class="reference external" href="https://books.google.com/books/about/Physical_Biology_of_the_Cell.html">3</a>, and <a class="reference external" href="https://books.google.com/books/about/Biophysics.html">4</a>, talks (<a class="reference external" href="https://www.youtube.com/watch?v=ioA1yuIA-t8">1</a>, <a class="reference external" href="https://www.youtube.com/watch?v=_cJ6k5R5RjU&amp;t=914s">2</a>, and <a class="reference external" href="https://www.youtube.com/watch?v=cT855rpX8bc">3</a>), and papers on this topic. As is the theme of this course, we aren’t going to go into the details of topics related to biology, physics, or algorithms too deeply. We want to help you explore and extract information from data. We introduce chemotaxis here because it is a particularly clean biological system that produces data that can be analyzed using some of the tools we have already taught you. So, as a reference for the assignments, here is a very quick summary of the overall phenomenon.</p>
<p>Bacteria need to navigate the chemical landscape they inhabit - they need to move towards stuff they like, and away from stuff they don’t. This is called chemotaxis: chemically driven motion. In particular, if you place a source of sugar in a solution with bacteria, say a sugar cube, and the bacteria will move towards the sugar. While seemingly simple, over 50 years of genetics, molecular biology, and biophysics has gone into the study of this one phenomenon. Why? Not because its the most crucial biological phenomenon but because it’s a system where incredibly precise measurements and experiments can be performed. Fortunately, what we have learned have been incredibly general principals. Indeed, I could (and have) given an entire course in biophysics that solely focused on bacterial chemotaxis.</p>
<p>So, how do bacteria move in response to chemicals? As shown in Figure <a class="reference internal" href="#fig-chem1"><span class="std std-numref">Fig. 14</span></a>, they have oars, called <strong>flagella</strong>, that propel them through their surroundings. If you watch a single bacterium move you will see that they use their oars in a funny manner resulting in a path that looks like a series of straight lines interjected by abrupt turns, as shown in Figure <a class="reference internal" href="#fig-chem1"><span class="std std-numref">Fig. 14</span></a>. This particular method of movement is known as making <strong>runs and tumbles</strong>. The way in which the bacteria moves <em>towards</em> a sugar crystal is that on the occasions when the runs happen to point <em>up</em> the sugar gradient they tend to be longer.  When the bacteria is pointed <em>down</em> the sugar gradient, the runs are shorter.  This simple mechanism ensures that given random orientations after a tumble, the bacteria will have a net movement, averaged over the many runs and tumbles, towards the source of sugar.</p>
<p>So, what produces the runs and tumbles? Indeed the microscopic mechanism for this is rather straightforward. The bacteria has many oars, but instead of them being like the oars for a rowboat, they look like corkscrews. This may seem like a strange choice of oars but there is a very deep reason for this design (if you find this interesting make sure to delve into the books cited above, as well as this beautiful <a class="reference external" href="https://science.curie.fr/wp-content/uploads/2016/04/Purcell_life_at_low_reynolds_number_1977.pdf">paper</a>). Crucially then, a single bacteria has many of these corkscrew-oars. When the corkscrews spin in one direction (counter clockwise to the direction of motion) then bacteria moves in straight line, making a run. If they rotate clockwise the bacteria tumbles in place. This is shown in <a class="reference internal" href="#fig-runtumble"><span class="std std-numref">Fig. 15</span></a>.</p>
<p>At this point then, the question is how a bacteria “knows” that it is moving up the gradient, and thus lengthen its straight line motion? Again, the answer to this involved around 2-3 decades worth of research, but one of the central experiments done to figure this out is what you will analyze as your first data set!</p>
<p>What researchers had difficulty with was in prescribing precise concentration profiles of chemicals and recording how bacteria modulated the frequency with which they did runs vs tumbles. So, to make the observations easier, some researchers chopped off the ends of the long flagella, and rooted the bacateria at the base of a flagellum down onto a slide. Now the bacteria couldn’t move translationally, but when the flagella rotated counter-clockwise (CCW) the bacteria would spin clockwise (CW), and vice versa. Now that the bacteria was spatially fixed, they could then pipette in very precise spatial profiles of chemicals and watch how the bacteria, now stuck in place, would respond. Watching a <a class="reference external" href="https://www.youtube.com/watch?v=4hexn-DtSt4">video</a> of this is somewhat entertaining, but I hope you can appreciate how very simple experimental design choices can be the key to opening up decades worth of insights.</p>
<figure class="align-default" id="fig-chem1">
<img alt="../../_images/bacchem1.png" src="../../_images/bacchem1.png" />
<figcaption>
<p><span class="caption-number">Fig. 14 </span><span class="caption-text">The run-and-tumble trajectories of bacteria in two conditions: the absence of any stimulus (left) and in the presence of a positive stimulus gradient (right).  Although it’s a cartoon, the idea is that there will be more runs that are longer in the direction of the gradient, compared to the movement when there is no stimulus.</span><a class="headerlink" href="#fig-chem1" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<figure class="align-default" id="fig-runtumble">
<img alt="../../_images/runandtumble.png" src="../../_images/runandtumble.png" />
<figcaption>
<p><span class="caption-number">Fig. 15 </span><span class="caption-text">An illustration of the physical arrangement of the flagella when they are rotated clockwise and counter-clockwise.</span><a class="headerlink" href="#fig-runtumble" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<!-- \begin{figure}
\centering
\captionsetup{width=0.8\linewidth}
\includegraphics[width=0.48\linewidth]{Pictures/bacchem1.png}
\includegraphics[width=0.48\linewidth]{runandtumble.png}
\caption{The left two panels illustrate the run-and-tumble trajectories of bacteria in two conditions: the absence of any stimulus (left) and in the presence of a positive stimulus gradient (right).  Although it's a cartoon, the idea is that there will be more runs that are longer in the direction of the gradient, compared to the movement when there is no stimulus.  The right part of the figure illustrates the physical arrangement of the flagella when they are rotated clockwise and counter-clockwise.}
\label{fig:chem1}
\end{figure} -->
<p>The <a class="reference external" href="https://www.dropbox.com/s/qabiwlk8nihq4jw/omega.txt?dl=0">data</a> that you will be working with is the angular velocity of a bacteria that has been stuck to a slide. Your task in assignment 1 will be in part to verify that you see the bacteria attempting to run and tumble, and to show this phenomenon in a meaningful way. This data set has been taken from and some  of the questions in the assignment have been inspired by William Bialek’s <a class="reference external" href="https://www.amazon.com/Biophysics-Searching-Principles-William-Bialek/dp/0691138915">book</a> on Biophysics, which I would thoroughly recommend for anyone interested in a more physical viewpoint of biology.  Additionally,  there is an entire <a class="reference external" href="https://northwestern.box.com/s/8rop5f6e679c0l0ymxcyp8m1023adyyf">set of notes</a> we have written on chemotaxis that gives a lot more biological and physical background on the topic. Check it out if you are interested!</p>
</section>
<section id="review">
<h2>Review<a class="headerlink" href="#review" title="Permalink to this headline">#</a></h2>
<p>So what was the point of this module? And why should you be reading this? How is this going to help you analyze your data?</p>
<p>A lot of the material here might be something you’ve seen before. However, getting some definitions and notations down is important. So I hope you have a more precise sense for what <span class="math notranslate nohighlight">\(P(X=x)\)</span> is saying. And, for example, how the conditional, marginal, and joint distributions are related to each other. Repeat these concepts in your mind over and over again, with simple examples (tossing two coins, drawing two cards with replacement from a pack, etc.) to really build intuition. I cannot emphasize enough that having these concepts deeply ingrained in your mind will serve you well.</p>
<p>Another really important idea was that even for something as simple as a coin flip you expect a variance in outcomes – a distribution. This makes estimating important features (which we introduced you to, such as the expectation or variance) from empirical data distributions challenging. At a more philosophical level, every statistic (quantity of interest), can only be determined to some finite precision from real data. You saw this in the simple example of coin flipping.</p>
<p>Please reflect on the depth of the central limit theorem within the context of coin flipping. I assure you that this fundamental result is playing a very important role in your data.  Intuitively the CLT tells us why getting more data makes the accuracy of any statistic of your distribution better. Additionally, the CLT says that when you have lots of data then often the distribution becomes increasingly “well-behaved” – which means, it becomes increasingly Gaussian. Why do we say that Gaussian distributions are “well-behaved”? Because you can entirely describe a Gaussian distribution using only two numbers – its mean and its variance. And so you need only report two numbers to tell us everything we need to know about the distribution. Generically, the data you generate or want to analyze is actually not Gaussian, or you don’t have enough data, which makes characterizing it far more challenging, but we will teach you how to deal with this.</p>
<p>Finally, we briefly introduced a Bayesian point of view to give you some experience with the concepts of the prior, likelihood, and posterior distributions. These 3 can be combined via Bayes’ Theorem, which then allows one to exactly state your prior beliefs (the ones you held before you had any data), and update them in the face of new data. What is also beautiful about the Bayesian approach is that any parameter (the probability of heads, even) is now explicitly modeled in a distributional sense. As we’ll see in the next module, you won’t just get a single number for your estimate but always the whole probability distribution. Of course the price you have to pay is that you almost always need a theoretical formula for the likelihood function. (For coin flips we have the binomial distribution.)  You’ll continue to encounter the Bayesian perspective as you continue in the course.</p>
</section>
<section id="learning-goals">
<h2>Learning Goals<a class="headerlink" href="#learning-goals" title="Permalink to this headline">#</a></h2>
<!-- Below is the curriculum alignment table for this whole module, showing where you can learn about some of the higher level learning goals across the course materials.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.98\linewidth]{Module1_CurriculumAlignment_JustTable.pdf}
\label{fig:CurrAlign}
\end{figure} -->
<p>To keep your expectations and goals in the right place, these are the skills and concepts we hope you have gained in the course of reading these notes, completing all the worksheets, and completing the assignment.  After this module, you should be able to:</p>
<ul class="simple">
<li><p>Theory</p>
<ul>
<li><p>Describe the concept of probability and random variables.</p></li>
<li><p>Interpret the notation <span class="math notranslate nohighlight">\(P(X=x)\)</span>.</p></li>
<li><p>Define and interpret the joint, conditional and marginal distributions of two random variables.</p></li>
<li><p>Discuss theoretical and empirical probability distributions and the differences between them.</p></li>
<li><p>Explain why <span class="math notranslate nohighlight">\(\int_{x\in X}P(X=x)dx = 1\)</span></p></li>
<li><p>Determine when two random variables are independent.</p></li>
<li><p>Discuss the Central Limit Theorem and its assumptions.  Describe the <span class="math notranslate nohighlight">\(\sqrt{N}\)</span> rule.</p></li>
<li><p>Discuss Bayes’ Theorem and identify the components of Equation <a class="reference internal" href="#equation-eqn-bayes">(7)</a></p></li>
<li><p>Discuss and interpret the Binomial Formula.</p></li>
<li><p>Discuss and interpret the definition of probability density functions and cumulative density functions.</p></li>
</ul>
</li>
<li><p>Calculations</p>
<ul>
<li><p>Given some data (or a theoretical distribution), calculate: <span class="math notranslate nohighlight">\(P(X=x)\)</span>, <span class="math notranslate nohighlight">\(P(X\leq x)\)</span>, <span class="math notranslate nohighlight">\(\sum_{x\in X}P(X=x)\)</span>.</p></li>
<li><p>Given data or a distribution, calculate <span class="math notranslate nohighlight">\(E[X]\)</span>, <span class="math notranslate nohighlight">\(Var[X]\)</span>, and other moments.</p></li>
<li><p>Given data or a distribution, calculate the median, IQR, or other percentiles.</p></li>
<li><p>Given data or a distribution, calculate the mode.</p></li>
</ul>
</li>
<li><p>Visualize</p>
<ul>
<li><p>Plot empirical or theoretical PDFs and CDFs.</p></li>
<li><p>Use vertical or horizontal lines to show means, medians, and percentiles on PDFs and CDFs.</p></li>
<li><p>Show how different quantities change as a function of (simulated) parameters.</p></li>
</ul>
</li>
<li><p>Simulations and Coding</p>
<ul>
<li><p>Simulate coin tosses with arbitrary <span class="math notranslate nohighlight">\(N\)</span> and <span class="math notranslate nohighlight">\(p\)</span>.</p></li>
<li><p>Implement Bayes’ Theorem to calculate a posterior distribution.</p></li>
<li><p>Extract time intervals from a time series (Assignment 1).</p></li>
<li><p>Generate PDFs and CDFs from data.</p></li>
</ul>
</li>
</ul>
<p>In the worksheets and assignments you will implement and grapple with all of these ideas. Enjoy!</p>
</section>
<section id="other-details">
<h2>Other Details<a class="headerlink" href="#other-details" title="Permalink to this headline">#</a></h2>
<section id="more-probability-rules">
<h3>More Probability Rules<a class="headerlink" href="#more-probability-rules" title="Permalink to this headline">#</a></h3>
<p>Consider two random variables <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span>.  Let <span class="math notranslate nohighlight">\(a\)</span> and <span class="math notranslate nohighlight">\(b\)</span> be specific outcomes of <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span>, respectively.  The following are some mathematical rules that you can use to manipulate probabilities.  These are absolutely not necessary to have memorized or accessible to be successful in this course, but thinking about them might help you gain some intuition about probabilities.</p>
<section id="the-addition-rule">
<h4>The Addition Rule<a class="headerlink" href="#the-addition-rule" title="Permalink to this headline">#</a></h4>
<p>If we want to know the likelihood of either of two outcomes occurring, we can use the <strong>addition rule</strong>.</p>
<div class="math notranslate nohighlight">
\[    P(A{=}a \text{ OR } B{=}b) = P(A{=}a) + P(B{=}b) - P(A{=}a \text{ AND } B{=}b)\]</div>
</section>
<section id="the-multiplication-rule">
<h4>The Multiplication Rule<a class="headerlink" href="#the-multiplication-rule" title="Permalink to this headline">#</a></h4>
<p>We have already seen this in the notes, but this rule is sometimes called the <strong>multiplication rule</strong>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}    P(A{=}a\text{ AND } B{=}b) &amp;= P(A{=}a)\times P(B{=}b | A{=}a)\\ 
    &amp;= P(B{=}b)\times P(A{=}a| B{=}b).\end{split}\]</div>
<p>Notice the symmetry in the way this relation can be written.</p>
</section>
<section id="the-complement-rule">
<h4>The Complement Rule<a class="headerlink" href="#the-complement-rule" title="Permalink to this headline">#</a></h4>
<p>This should be an obvious consequence of the fact that <span class="math notranslate nohighlight">\(\sum_{x\in X}P(X{=}x) = 1\)</span> (the likelihood of <em>something happening</em> is 100%), but this is formally called the <strong>complement rule</strong>.</p>
<div class="math notranslate nohighlight">
\[    P(A{\neq}a) = 1 - P(A{=}a)\]</div>
</section>
<section id="law-of-total-probability">
<h4>Law of Total Probability<a class="headerlink" href="#law-of-total-probability" title="Permalink to this headline">#</a></h4>
<p>We can always break probabilities of one random variable into conditional statements that depend on other random variables.  This is explicitly done with the <strong>Law of Total Probability</strong>.</p>
<div class="math notranslate nohighlight">
\[    P(A{=}a) = P(A{=}a|B{=}b)\cdot P(B{=}b) + P(A{=}a|B{\neq}b)\cdot P(B{\neq}b)\]</div>
<p>That is, the probability that <span class="math notranslate nohighlight">\(A=a\)</span> can be found conditionally by considering how likely it is when <span class="math notranslate nohighlight">\(B=b\)</span> and adding the likelihood that arises when <span class="math notranslate nohighlight">\(B\neq b\)</span>.</p>
</section>
<section id="marginal-distributions">
<h4>Marginal Distributions<a class="headerlink" href="#marginal-distributions" title="Permalink to this headline">#</a></h4>
<p>This previous rule suggests that we can extract probabilities of one random variable from a joint distribution.  Indeed, if we want to know the probability distribution of <span class="math notranslate nohighlight">\(A\)</span> without consideration of <span class="math notranslate nohighlight">\(B\)</span>, we can add up the way that <span class="math notranslate nohighlight">\(A\)</span> is distributed over all the different outcomes of <span class="math notranslate nohighlight">\(B\)</span>.  This is called <strong>marginalization</strong> and will come up later in the course.  More concretely, if we have a joint distribution <span class="math notranslate nohighlight">\(P(A, B)\)</span>, then we say that <span class="math notranslate nohighlight">\(P(A)\)</span> and <span class="math notranslate nohighlight">\(P(B)\)</span> are <strong>marginal distributions</strong> of <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span>.  We can find these quantities from the joint using a summation (or integral):</p>
<div class="amsmath math notranslate nohighlight" id="equation-3fc965dc-1ca8-4e7f-af4d-6ff162d81d1a">
<span class="eqno">(9)<a class="headerlink" href="#equation-3fc965dc-1ca8-4e7f-af4d-6ff162d81d1a" title="Permalink to this equation">#</a></span>\[\begin{align}
    P(A) &amp;= \sum_{b\in B}P(A, B)\\
    P(B) &amp;= \sum_{a\in A}P(A, B)
\end{align}\]</div>
</section>
</section>
<section id="more-theoretical-distributions">
<h3>More Theoretical Distributions<a class="headerlink" href="#more-theoretical-distributions" title="Permalink to this headline">#</a></h3>
<p>For the sake of clarity and brevity we ignored introducing the specific functional forms of some relevant and interesting distributions.  We list them here with some relevant notes.  Again, it is not necessary to memorize any of these formulas, and we will remind you of any relevant details as we go forward.</p>
<p>As a note about notation, probability distributions generally have <strong>parameters</strong> that determine their shape and moments.  When we have a distribution of some quantity, <span class="math notranslate nohighlight">\(x\)</span>, and it has parameters <span class="math notranslate nohighlight">\(\theta_1, \theta_2, \ldots\)</span>, we will generally notate this distribution as</p>
<div class="math notranslate nohighlight">
\[P(x;\theta_1, \theta_2,\ldots)\qquad \ProbOr\qquad 
P(x|\theta_1, \theta_2,\ldots).\]</div>
<p>It may seem odd that we’re re-using the notation for conditional probabilities, but you can convince yourself that the notation we’re trying to express actually <em>is</em> a conditional probability – we’re conditioning on the different parameters.</p>
<section id="the-binomial-distribution">
<h4>The Binomial Distribution<a class="headerlink" href="#the-binomial-distribution" title="Permalink to this headline">#</a></h4>
<p>We have already discussed this extensively in the text, but the <strong>binomial distribution</strong> describes the distribution of the number of <strong>successes</strong>, <span class="math notranslate nohighlight">\(k\)</span>, that occur in <span class="math notranslate nohighlight">\(N\)</span> trials, where the likelihood of a single success is <span class="math notranslate nohighlight">\(p\)</span>.  If a “success” is a coin coming up heads, then this distribution describes the number of heads in <span class="math notranslate nohighlight">\(N\)</span> flips of a coin.</p>
<p>The formula for the PDF is given by Equation <a class="reference internal" href="#equation-eqn-binomeqn">(3)</a>, which we repeat here:</p>
<div class="math notranslate nohighlight">
\[    P_{Binom}(k;N, p) = {N\choose k}p^k(1-p)^{N-k}.\]</div>
<p>In the <code class="docutils literal notranslate"><span class="pre">scipy.stats</span></code> package, you can access the binomial distribution with the <code class="docutils literal notranslate"><span class="pre">scipy.stats.binom</span></code> object.</p>
</section>
<section id="the-gaussian-distribution">
<h4>The Gaussian Distribution<a class="headerlink" href="#the-gaussian-distribution" title="Permalink to this headline">#</a></h4>
<p>Perhaps the most important distribution ever, the <strong>Gaussian</strong> or <strong>Normal</strong> distribution describes a wide variety of phenomena.  We’ll discuss it increasingly as the course progresses, but for now we’ll just give some basic facts.</p>
<p>Normal distributions are parameterized by their mean, <span class="math notranslate nohighlight">\(\mu\)</span>, and variance, <span class="math notranslate nohighlight">\(\sigma^2\)</span>, which should be interesting on its own.  The PDF is given by</p>
<div class="math notranslate nohighlight">
\[    P_{Gauss}(x;\mu, \sigma^2) =
    \mathcal{N}\left(x;\mu,\sigma^2\right) = 
    \frac{1}{\sqrt{2\pi \sigma^2}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}.\]</div>
<p>When <span class="math notranslate nohighlight">\(\mu=0\)</span> and <span class="math notranslate nohighlight">\(\sigma^2=1\)</span>, the distribution is referred to as the <strong>standard normal distribution</strong>.  You can convert any normal distribution to a standard normal by making the substitution <span class="math notranslate nohighlight">\(y = (x - \mu)/\sigma\)</span> (subtracting the mean and dividing by the standard deviation).  This is known as <strong>standardization</strong>.</p>
<p>In the <code class="docutils literal notranslate"><span class="pre">scipy.stats</span></code> package, you can access the binomial distribution with the <code class="docutils literal notranslate"><span class="pre">scipy.stats.norm</span></code> object.</p>
</section>
<section id="the-poisson-distribution">
<h4>The Poisson Distribution<a class="headerlink" href="#the-poisson-distribution" title="Permalink to this headline">#</a></h4>
<p>The Poisson distribution is another useful distribution to know about as it arises frequently in natural processes.  It describes the distribution of events that occur in a given time interval, if the average number of events in an interval is <span class="math notranslate nohighlight">\(\lambda\)</span> (also known as the arrival <strong>rate</strong>), and each event occurs independently of the last.  The Poisson distribution can be considered to be a limiting case of the binomial distribution when the probability of success is very small.</p>
<p>The PDF of the Poisson distribution is</p>
<div class="math notranslate nohighlight">
\[    P_{Poisson}(k;\lambda) = \frac{\lambda^ke^{-\lambda}}{k!}\]</div>
<p>In the <code class="docutils literal notranslate"><span class="pre">scipy.stats</span></code> package, you can access the binomial distribution with the <code class="docutils literal notranslate"><span class="pre">scipy.stats.poisson</span></code> object.</p>
</section>
<section id="the-exponential-distribution">
<h4>The Exponential Distribution<a class="headerlink" href="#the-exponential-distribution" title="Permalink to this headline">#</a></h4>
<p>Finally, the <strong>exponential distribution</strong> describes random variables whose PDFs are exponential functions.  You will explore the exponential distribution extensively in the assignments, so for now we will only say that the exponential distribution can be  used to describe the time interval between Poisson-distributed events.</p>
<p>The PDF of the exponential distribution is</p>
<div class="math notranslate nohighlight">
\[\begin{split}    P_{Expon}(x;\lambda) = \begin{cases}
        \lambda e^{-\lambda x} &amp;\qquad x\geq 0\\
        0 &amp; \qquad x &lt; 0
    \end{cases}\end{split}\]</div>
<p>In the <code class="docutils literal notranslate"><span class="pre">scipy.stats</span></code> package, you can access the binomial distribution with the <code class="docutils literal notranslate"><span class="pre">scipy.stats.expon</span></code> object.</p>
</section>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./CourseFiles/Module_1"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="Module_1.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Module 1: The Basics</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="Worksheet_1_1_CoinFlipping.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Worksheet 1.1: Coin Tossing</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Eric Johnson<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>